{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4a0d04-a52a-4c04-aa33-d5bd4334b3f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 101\u001b[0m\n\u001b[0;32m     97\u001b[0m     ep \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(episodes)\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeak_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration_min\u001b[39m\u001b[38;5;124m\"\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ep\n\u001b[0;32m    100\u001b[0m episodes \u001b[38;5;241m=\u001b[39m extract_bn_episodes(\n\u001b[1;32m--> 101\u001b[0m     dfA,\n\u001b[0;32m    102\u001b[0m     time_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_s\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    103\u001b[0m     flag_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_bn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    104\u001b[0m     prod_cols\u001b[38;5;241m=\u001b[39mprod_rate_cols,\n\u001b[0;32m    105\u001b[0m     score_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbottleneck_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    106\u001b[0m     min_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    107\u001b[0m     merge_gap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    108\u001b[0m )\n\u001b[0;32m    110\u001b[0m episodes\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfA' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_bn_episodes(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str = \"time_s\",          # or \"tick\" if you prefer\n",
    "    flag_col: str = \"is_bn\",\n",
    "    prod_cols: list[str] = None,\n",
    "    score_col: str = \"bottleneck_score\",\n",
    "    min_len: int = 2,                  # minimum episode duration in minutes\n",
    "    merge_gap: int = 0                 # merge episodes separated by <= merge_gap non-bn minutes\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    d = df.copy()\n",
    "    d = d.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    if prod_cols is None:\n",
    "        prod_cols = [c for c in d.columns if c.startswith(\"production__\") and c.endswith(\"_per_min\")]\n",
    "    if not prod_cols:\n",
    "        raise ValueError(\"No production__*_per_min columns found. Pass prod_cols explicitly.\")\n",
    "\n",
    "    d[flag_col] = d[flag_col].fillna(False).astype(bool)\n",
    "\n",
    "    # run boundaries\n",
    "    x = d[flag_col].to_numpy()\n",
    "    start_of_run = np.r_[True, x[1:] != x[:-1]]\n",
    "    run_id = start_of_run.cumsum()\n",
    "    d[\"_run\"] = run_id\n",
    "\n",
    "    # raw bn runs\n",
    "    runs = (\n",
    "        d[d[flag_col]]\n",
    "        .groupby(\"_run\", as_index=False)\n",
    "        .agg(\n",
    "            start=(time_col, \"min\"),\n",
    "            end=(time_col, \"max\"),\n",
    "            minutes=(time_col, \"size\"),\n",
    "        )\n",
    "        .sort_values(\"start\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # optional merge by small gaps\n",
    "    if merge_gap > 0 and len(runs) > 0:\n",
    "        merged = []\n",
    "        cur = runs.loc[0].to_dict()\n",
    "        for i in range(1, len(runs)):\n",
    "            nxt = runs.loc[i].to_dict()\n",
    "            gap = int(nxt[\"start\"] - cur[\"end\"]) - 1  # time_col is in seconds or ticks? see note below\n",
    "            if gap <= merge_gap:\n",
    "                cur[\"end\"] = max(cur[\"end\"], nxt[\"end\"])\n",
    "                cur[\"minutes\"] = None  # recompute later\n",
    "            else:\n",
    "                merged.append(cur)\n",
    "                cur = nxt\n",
    "        merged.append(cur)\n",
    "        runs = pd.DataFrame(merged)\n",
    "\n",
    "    # convert time units into minutes properly\n",
    "    # time_s seems to be in seconds. Your data is per minute so step is 60.\n",
    "    step = int(pd.Series(d[time_col].diff().dropna()).mode().iloc[0]) if len(d) > 1 else 60\n",
    "    if step <= 0:\n",
    "        step = 60\n",
    "\n",
    "    # finalize episodes with dominant item computed over the whole episode\n",
    "    episodes = []\n",
    "    for ep_id, r in enumerate(runs.itertuples(index=False), start=1):\n",
    "        mask = (d[time_col] >= r.start) & (d[time_col] <= r.end) & d[flag_col]\n",
    "        chunk = d.loc[mask]\n",
    "\n",
    "        # duration in minutes based on step\n",
    "        duration_min = int((chunk[time_col].max() - chunk[time_col].min()) / step) + 1 if len(chunk) else 0\n",
    "        if duration_min < min_len:\n",
    "            continue\n",
    "\n",
    "        # dominant item for the episode based on summed production during the episode\n",
    "        sums = chunk[prod_cols].sum(axis=0)\n",
    "        total = float(sums.sum())\n",
    "        dom_item = str(sums.idxmax()) if total > 0 else None\n",
    "        dom_share = float(sums.max() / total) if total > 0 else np.nan\n",
    "\n",
    "        peak = float(chunk[score_col].max()) if score_col in chunk.columns and len(chunk) else np.nan\n",
    "        mean = float(chunk[score_col].mean()) if score_col in chunk.columns and len(chunk) else np.nan\n",
    "\n",
    "        episodes.append({\n",
    "            \"episode_id\": ep_id,\n",
    "            \"start\": int(chunk[time_col].min()),\n",
    "            \"end\": int(chunk[time_col].max()),\n",
    "            \"duration_min\": duration_min,\n",
    "            \"dominant_item\": dom_item,\n",
    "            \"dominant_share\": dom_share,\n",
    "            \"peak_score\": peak,\n",
    "            \"mean_score\": mean,\n",
    "            \"bn_minutes\": int(len(chunk))\n",
    "        })\n",
    "\n",
    "    ep = pd.DataFrame(episodes).sort_values([\"peak_score\",\"duration_min\"], ascending=[False, False]).reset_index(drop=True)\n",
    "    return ep\n",
    "\n",
    "episodes = extract_bn_episodes(\n",
    "    dfA,\n",
    "    time_col=\"time_s\",\n",
    "    flag_col=\"is_bn\",\n",
    "    prod_cols=prod_rate_cols,\n",
    "    score_col=\"bottleneck_score\",\n",
    "    min_len=2,\n",
    "    merge_gap=0\n",
    ")\n",
    "\n",
    "episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb2f95f-a313-40e4-bd01-078465fc11d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
